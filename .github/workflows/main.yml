# name: Daily Multi-Source Scraper

# This triggers the workflow on a schedule
# on:
  # Runs the job at 14:00 UTC (which is 16:00 CEST/Danish Summer Time) every day.
  # Note: Local time will be 15:00 CET/Danish Winter Time.
#  schedule:
#    - cron: '0 14 * * *'
  
  # Allows manual runs from the Actions tab
#  workflow_dispatch:

#jobs:
#  scrape-and-commit:
#    runs-on: ubuntu-latest
#    steps:
      # Step 1: Checks out your repository under $GITHUB_WORKSPACE, so your job can access it
#      - name: Checkout repository
#        uses: actions/checkout@v4

      # Step 2: Sets up a Python environment for use in the workflow
#      - name: Set up Python
#        uses: actions/setup-python@v5
#        with:
#          python-version: '3.11'

      # Step 3: Installs the Python packages required by your script
 #     - name: Install dependencies
 #       run: |
 #         python -m pip install --upgrade pip
 #         pip install requests beautifulsoup4 lxml 

      # Step 4: Runs the Python script
#      - name: Run Python script
#        run: python scrape.py

      # Step 5: Uses a dedicated action to commit the changes back to the repository
      # Assuming your script creates/updates 'database.db' in the root directory
#      - name: Commit and push changes
#        uses: stefanzweifel/git-auto-commit-action@v5
#        with:
#          commit_message: "Automated daily scrape update"
          # Commit the main database file generated by your script
#          file_pattern: 'database.db'
